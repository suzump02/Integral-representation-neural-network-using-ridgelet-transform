@article{SONODA2017233,
title = {Neural network with unbounded activation functions is universal approximator},
journal = {Applied and Computational Harmonic Analysis},
volume = {43},
number = {2},
pages = {233-268},
year = {2017},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1063520315001748},
author = {Sho Sonoda and Noboru Murata},
keywords = {Neural network, Integral representation, Rectified linear unit (ReLU), Universal approximation, Ridgelet transform, Admissibility condition, Lizorkin distribution, Radon transform, Backprojection filter, Bounded extension to },
abstract = {This paper presents an investigation of the approximation property of neural networks with unbounded activation functions, such as the rectified linear unit (ReLU), which is the new de-facto standard of deep learning. The ReLU network can be analyzed by the ridgelet transform with respect to Lizorkin distributions. By showing three reconstruction formulas by using the Fourier slice theorem, the Radon transform, and Parseval's relation, it is shown that a neural network with unbounded activation functions still satisfies the universal approximation property. As an additional consequence, the ridgelet transform, or the backprojection filter in the Radon domain, is what the network learns after backpropagation. Subject to a constructive admissibility condition, the trained network can be obtained by simply discretizing the ridgelet transform, without backpropagation. Numerical examples not only support the consistency of the admissibility condition but also imply that some non-admissible cases result in low-pass filtering.}
}

@article{Sonoda2023AppliedMath,
  title={Integral Representation Neural Networks and Ridgelet Transform [in Japanese]},
  author={Sonoda, Sho}, 
  journal={Applied Mathematics (Oyo Sugaku)}, 
  volume={33},
  number={1},
  pages={4-13},
  year={2023},
    doi={10.11540/bjsiam.33.1_4}
  }